##### CPU亲和性



###### 软亲和性

 就是进程要在指定的 CPU 上尽量长时间地运行而不被迁移到其他处理器，Linux 内核进程调度器天生就具有被称为 软 CPU 亲和性（affinity） 的特性，这意味着进程通常不会在处理器之间频繁迁移。这种状态正是我们希望的，因为进程迁移的频率小就意味着产生的负载小。

###### 硬亲和性

简单来说就是利用linux内核提供给用户的API，强行将进程或者线程绑定到某一个指定的cpu核运行。





在linux内核中，所有的进程都有一个相关的数据结构，称为 task_struct。这个结构非常重要，原因有很多；其中与 亲和性（affinity）相关度最高的是 cpus_allowed 位掩码。这个位掩码由 n 位组成，与系统中的 n 个逻辑处理器一一对应。 具有 4 个物理 CPU 的系统可以有 4 位。如果这些 CPU 都启用了超线程，那么这个系统就有一个 8 位的位掩码。 如果为给定的进程设置了给定的位，那么这个进程就可以在相关的 CPU 上运行。因此，如果一个进程可以在任何 CPU 上运行，并且能够根据需要在处理器之间进行迁移，那么位掩码就全是 1。实际上，这就是 Linux 中进程的缺省状态；

```
struct task_struct {
	...
	cpumask_t cpus_allowed;  //位掩码
	...
}
```



一个CPU的亲合力掩码用一个cpu_set_t结构体来表示一个CPU集合,下面的几个宏分别对这个掩码集进行操作:
`·CPU_ZERO()` 清空一个集合
`·CPU_SET()`与`CPU_CLR()`分别对将一个给定的CPU号加到一个集合或者从一个集合中去掉.
`·CPU_ISSET()`检查一个CPU号是否在这个集合中.

下面两个函数就是用来设置获取线程CPU亲和力状态:
`·sched_setaffinity(pid_t pid, unsigned int cpusetsize, cpu_set_t *mask)`
该函数设置进程为pid的这个进程,让它运行在mask所设定的CPU上.如果pid的值为0,则表示指定的是当前进程,使当前进程运行在mask所设定的那些CPU上.第二个参数`cpusetsize`是mask所指定的数的长度.通常设定为`sizeof(cpu_set_t)`.如果当前pid所指定的进程此时没有运行在mask所指定的任意一个CPU上,则该指定的进程会从其它CPU上迁移到mask的指定的一个CPU上运行.
`sched_getaffinity(pid_t pid, unsigned int cpusetsize, cpu_set_t *mask)`
该函数获得pid所指示的进程的CPU位掩码,并将该掩码返回到mask所指向的结构中.即获得指定pid当前可以运行在哪些CPU上.同样,如果pid的值为0.也表示的是当前进程

而mask的表现是如此的：如果是0X23，转换成二进制则为00100011,则表明进程绑定在0核、1核和5核上。绑核需要注意是，子进程会继承父进程的绑核关系。


```
  /* set process affinity for main thread */
  CPU_ZERO (&cpuset);            //cpu_set_t cpuset;
  CPU_SET (main_core, &cpuset);  // int main_core = 1;
  pthread_setaffinity_np (pthread_self (), sizeof (cpu_set_t), &cpuset);
```



通过`cat /proc/PID/status`，可以查看线程的状态

```
Cpus_allowed:   00000000,00000000,00000000,00000003
Cpus_allowed_list:      0-1
```



##### 系统调用

　syscall是执行一个系统调用，根据指定的参数number和所有系统调用的接口来确定调用哪个系统调用，用于用户空间跟内核之间的数据交换

```
 cpus = sysconf(_SC_NPROCESSORS_CONF);
```

##### 字节对齐

第一个原因——很多CPU只从对齐的地址开始加载数据，而有的CPU这样做，只是更快一点。
第二个原因——外部总线从内存一次获取的数据往往不是1byte，而是4bytes或许8bytes,或者更多~~
这意味着CPU不是一次仅仅抓取一个byte，而是很4个或者8个byte。
这样的话，假设你请求2到3bytes的内容，其实CPU一次抓取了4或8byte的内容。

这两个原因和起来的话，就很明显了。

假如说你要获取8byte从9开始，这个时候问题就来了。
CPU必须从8开始获取8bytes的内容，然后再从16开始，再获取8Byte的内容。
而如果你是从8开始的话，只需要从8开始获取8bytes的内容即可。
这里有两个结构体，你可以体会下他们的区别。

```
Struct A
{
    Int a;
    short b;
    int c;
    char d;
};
struct B{
    Int a ;
    int c;
    short b;
    char d;
};
```

不过现在，也只有写比较底层的时候，才会考虑这些。

你若写C的话，这些了解一下是挺好的。

像脚本语言，这些就很少考虑了。



在实际的gcc编译中，大小有区别

```
typedef struct {
	int pad1;
	char b;         // 1
}vec_s_t1;

typedef struct {
	long pad1;
	char b;         // 1
}vec_s_t2;

typedef struct {
    char pad1;
    char b;         // 1
}vec_s_t3;
```

字节以什么对齐，要看变量最大的是多少，首先确定cpu，如果是64位的那么long就是64，结构体vec_s_t2就是16字节。

int为4字节，vec_s_t1就是8字节。vec_s_t3是2字节。

**按照最大成员的长度来对齐**



##### cpu乱序执行



​	处理器基本上会按照程序中书写的机器指令的顺序执行。按照书写顺序执行称为按序执行（In-Order ）。按照书写顺序执行时，如果从内存读取数据的加载指令、除法运算指令等延迟（等待结果的时间）较长的指令后面紧跟着使用该指令结果的指令，就会陷入长时间的等待。尽管这种情况无可奈何，但有时，再下一条指令并不依赖于前面那条延迟较长的指令，只要有了操作数就能执行。

此时可以打乱机器指令的顺序，就算指令位于后边，只要可以执行，就先执行，这就是乱序执行（Out-of-Order）。

乱序执行时，由于数据依赖性而无法立即执行的指令会被延后，因此可以减轻数据灾难的影响。





##### 含参数的宏与函数



 宏替换不占运行时间，只占编译时间；而函数调用则占运行时间（分配单元、保留现场、值传递、返回），所以每次执行都要载入所以执行起来比较慢一些。。

*?*     定义宏的时候不要在宏及其参数之间键入空格，因为宏替换的时候会把你不经意打的空格当作宏的一部分进去。

*?*     在宏定义中把每个参数都用括号括起来，同时把整个结果也用括号（对于单个表达式的宏，可以使用小括号*(),*对于宏定义的复合语句可以使用*{}*，*Linux*内核中有一个比较好的宏定义，*do {…}while(0)*）括起来，以防止当宏用于一个更大的表达式时可能出现的问题。

*?*     使用宏次数多时，宏展开后源程序长，因为每展开一次都使程序增长，但是执行起来比较快一点（这也不是绝对的，当有很多宏展开，目标文件很大，执行的时候运行时系统换页频繁，效率就会低下）。而函数调用不使源程序变长。

*?*     函数调用时，先求出实参表达式的值，然后带入形参。而使用带参的宏只是进行简单的字符替换。

*?*     函数调用是在程序运行时处理的，分配临时的内存单元；而宏展开则是在编译时进行的，在展开时并不分配内存单元，不进行值的传递处理，也没有*“*返回值*”*的概念。

*?*     对函数中的实参和形参都要定义类型，二者的类型要求一致，如不一致，应进行类型转换；而宏不存在类型问题，宏名无类型，它的参数也无类型，只是一个符号代表，展开时带入指定的字符即可。宏定义时，字符串可以是任何类型的数据。

*?*     调用函数只可得到一个返回值，且有返回类型，而宏没有返回值和返回类型，但是用宏可以设法得到几个结果。

*?*     函数体内有*Bug*，可以在函数体内打断点调试。如果宏体内有*Bug*，那么在执行的时候是不能对宏调试的，即不能深入到宏内部。

*?*     *C++*中宏不能访问对象的私有成员，但是成员函数就可以。

*?*     宏的定义很容易产生二义性，如：定义*#define S(a) (a)\*(a)*，代码*S(a++)*，宏展开变成*(a++)\*(a++)*这个大家都知道，在不同编译环境下会有不同结果。

***\**\*内联函数和宏的区别（内联函数的优点）\*\**\***

***\*内联函数和宏的区别在于，宏是由预处理器对宏进行替代，而内联函数是通过编译器控制来实现的。\****而且内联函数是真正的函数，只是在需要用到的时候，内联函数像宏一样的展开，**所以取消了函数的参数压栈，减少了调用的开销。**你可以象调用函数一样来调用内联函数，而不必担心会产生于处理宏的一些问题。

我们可以用*Inline*来定义内联函数，不过，任何在类的说明部分定义的函数都会被自动的认为是内联函数。（当然内联函数的识别对编译器来说是一个复杂的工作，后继可能有专门的论述）

当然，**内联函数也有一定的局限性。就是函数中的执行代码不能太多了**，如果，内联函数的函数体过大，一般的编译器会放弃内联方式，而采用普通的方式调用函数。这样，内联函数就和普通函数执行效率一样了。

内联函数通过避免被调用的开销来提高执行效率，尤其是它能够通过调用（*“*过程化集成*”*）被编译器优化。

***\*如何选择使用宏还是函数：以下情况可以选择宏，其他情况最好选用函数\****

*2*    一般来说，用宏来代表简短的表达式比较合适。

*2*    在考虑效率的时候，可以考虑使用宏，或者内联函数。

*2*    在头文件保护（防止重复包含编译），条件编译中的*#ifdef*，*#if defined*以及*assert*的实现。





##### 伪共享

###### 存储器结构

存储器层次结构

![img](http://file.elecfans.com/web1/M00/8E/2D/pIYBAFytsGmAGa8LAACKXM5Pgv4299.jpg)

**main memory** 主存

**fixed rigid disk**  硬盘

**optical disk**  光盘

**magnetic tape**  磁带



**寄存器的速度最快，可以在一个时钟周期内访问，其次是高速缓存，可以在几个时钟周期内访问，普通内存可以在几十个或几百个时钟周期内访问。**

![img](https://img-blog.csdn.net/20180606100636738?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIxMTI1MTgz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



存储器分级，利用的是局部性原理。我们可以以经典的阅读书籍为例。我在读的书，捧在手里（寄存器），我最近频繁阅读的书，放在书桌上（缓存），随时取来读。当然书桌上只能放有限几本书。我更多的书在书架上（内存）。如果书架上没有的书，就去图书馆（磁盘）。我要读的书如果手里没有，那么去书桌上找，如果书桌上没有，去书架上找，如果书架上没有去图书馆去找。可以对应寄存器没有，则从缓存中取，缓存中没有，则从内存中取到缓存，如果内存中没有，则先从磁盘读入内存，再读入缓存，再读入寄存器




###### 什么是cache

​	Cache Memory也被称为Cache，是存储器子系统的组成部分，存放着程序经常使用的指令和数据，这就是Cache的传统定义。从广义的角度上看，Cache是块设备为了缓解访问慢设备延时的预留的Buffer，从而可以在掩盖访问延时的同时，尽可能地提高数据传输率。 快和慢是一个相对概念，与微架构(Microarchitecture)中的 L1/L2/L3 Cache相比， DDR内存是一个慢速设备；在磁盘 I/O 系统中，DDR却是快速设备，在磁盘 I/O 系统中，仍在使用DDR内存作为磁介质的Cache。在一个微架构中，除了有L1/L2/L3 Cache之外，用于虚实地址转换的各级TLB， MOB( Memory Ordering Buffers)、在指令流水线中的ROB，Register File和BTB等等也是一种Cache。我们这里的Cache，是狭义 Cache，是CPU流水线和主存储器的 L1/L2/L3 Cache。





<img src="https://pic2.zhimg.com/80/v2-16ada06ec2a01cad09cc4d3c0fcc4b81_720w.jpg" alt="img" style="zoom: 100%;" />



现在：L3被加入到CPU Die中，它在逻辑上是共享模式。而L2则被每个Core单独占据。这时L2也常被指做**MLC**（Middle Level Cache），而L3也被叫做**LLC**（Last Level Cache）：



![img](https://pic3.zhimg.com/80/v2-dada31c65af46f4b82bfe3e8ed63fd32_720w.jpg)





Cache速度比内存速度快多少？
大家都知道内存都是DRAM，但对Cache是怎么组成就所知不多了。Cache是由CAM（Content Addressable Memory ）为主体的tag和SRAM组成的。我们今后在系列文章中会详细介绍CAM的组成，这里简单比较一下DRAM和SRAM。DRAM组成很简单：

![img](https://pic4.zhimg.com/80/v2-81eb423b81c9cdfb08e15a9cfef631c7_720w.jpg)

DRAM
只有一个晶体管和一个电容。而SRAM就复杂多了,一个简化的例子：

![img](https://pic2.zhimg.com/80/v2-7056f546c21dfa14654990a6b9f7c109_720w.jpg)

SRAM存储一位需要花6个晶体管，而DRAM只需要花一个电容和一个晶体管。cache追求的是速度所以选择SRAM，而内存则追求容量所以选择能够在相同空间中存放更多内容并且造价相对低廉的DRAM。

我们姑且不去讨论关于SRAM是如何静态存储数据（触发器）的。为什么DRAM需要不断刷新呢？

DRAM的数据实际上是存在电容里的。而电容放久了，内部的电荷就会越来越少，对外就形成不了电位的变化。而且当对DRAM进行读操作的时候需要将电容与外界形成回路，通过检查是否有电荷流进或流出来判断该bit是1还是0。所以无论怎样，在读操作中我们都破坏了原来的数据。所以在读操作结束后需要将数据写回DRAM中。在整个读或者写操作的周期中，计算机都会进行DRAM的刷新，通常是刷新的周期是4ms-64ms。

　　关于SRAM和DRAM的寻址方式也有所不同。虽然通常我们都认为内存像一个长长的数组呈一维排列，但实际上内存是以一个二维数组的形式排列的，每个单元都有其行地址和列地址，当然cache也一样。而这两者的不同在于对于容量较小的SRAM，我们可以将行地址和列地址一次性传入到SRAM中，而如果我们对DRAM也这样做的话，则需要很多很多根地址线（容量越大，地址越长，地址位数越多）。所以我们选择分别传送行地址和列地址到DRAM中。先选中一整行，然后将整行数据存到一个锁存器中，等待列地址的传送然后选中所需要的数据。这也是为什么SRAM比DRAM快的原因之一。















[](https://zhuanlan.zhihu.com/p/31422201)

###### 多核架构

随着多核架构的普及，对称多处理器 (SMP) 系统成为主流。例如，一个物理 CPU 可以存在多个物理 Core，而每个 Core 又可以存在多个硬件线程。 x86 以下图为例，1 个 x86 CPU 有 4 个物理 Core，每个 Core 有两个 HT (Hyper Thread)，



![img](http://file.elecfans.com/web1/M00/8E/2D/pIYBAFytsGmAcnE_AAEEpQvyaXA797.jpg)



从硬件的角度，上图的 L1 和 L2 Cache 都被两个 HT 共享，且在同一个物理 Core。而 L3 Cache 则在物理 CPU 里，被多个 Core 来共享。 而从 OS 内核角度，每个 HT 都是一个逻辑 CPU，因此，这个处理器在 OS 来看，就是一个 8 个 CPU 的 SMP 系统。

L1 i-cache因为只读不改，可以放在IFU(instructionfetch unit)一起，IFU可以直接读i-cache；
但是L1 d-cache因为可读写，控制复杂，IFU/EU访问d-cache需要通过bus-interface-bus,速度要比i-cache慢。





​	Cache Line 伪共享问题，就是由多个 CPU 上的多个线程同时修改自己的变量引发的。这些变量表面上是不同的变量，但是实际上却存储在同一条 Cache Line 里。 在这种情况下，由于 Cache 一致性协议，两个处理器都存储有相同的 Cache Line 拷贝的前提下，**本地 CPU 变量的修改会导致本地 Cache Line 变成 Modified 状态，然后在其它共享此 Cache Line 的 CPU 上， 引发 Cache Line 的 Invaidate 操作，导致 Cache Line 变为 Invalidate 状态，从而使 Cache Line 再次被访问时，发生本地 Cache Miss，从而伤害到应用的性能。** 在此场景下，多个线程在不同的 CPU 上高频反复访问这种 Cache Line 伪共享的变量，则会因 Cache 颠簸引发严重的性能问题。

下图即为两个线程间的 Cache Line 伪共享问题的示意图，

![img](http://file.elecfans.com/web1/M00/8E/2D/pIYBAFytsGyAFKiHAAEBnYXnMA4390.png)

###### 怎样解决伪共享

​	但是，要怎样才能知道一个应用是不是受伪共享所害呢？Joe Mario 提交的 patch 能够解决这个问题。Joe 的 patch 是在 Linux 的著名的 perf 工具上，添加了一些新特性，叫做 c2c， 意思是“缓存到缓存” (cache-2-cache)。

Redhat 在很多 Linux 的大型应用上使用了 c2c 的原型，成功地发现了很多热的伪共享的 Cache Line。 Joe 在博客里总结了一下 perf c2c 的主要功能：

- 发现伪共享的 Cache Line
- 谁在读写上述的 Cache Line，以及访问发生处的 Cache Line 的内部偏移
- 这些读者和写者分别的 pid, tid, 指令地址，函数名，二进制文件
- 每个读者和写者的源代码文件，代码行号
- 这些热点 Cache Line 上的，load 操作的平均延迟
- 这些 Cache Line 的样本来自哪些 NUMA 节点， 由哪些 CPU 参与了读写

perf c2c 和 perf 里现有的工具比较类似:

- 先用 perf c2c record 通过采样，收集性能数据
- 再用 perf c2c report 基于采样数据，生成报告

如果想了解 perf c2c 的详细使用，请访问: PERF-C2C(1)

这里还有一个完整的 perf c2c 的输出的样例。

最后，还有一个小程序的源代码，可以产生大量的 Cache Line 伪共享，用以测试体验: Fasle sharing .c src file



##### cache的哲学

###### 时间局限性

程序即将用到的指令/数据可能就是目前正在用到的指令/数据，因此当前用到的指令或数据将会继续放在cache中以备将来继续使用；如循环语句的终止条件满足之前，处理器会反复用到循环语句中的指令；
这就是双层循环时候为什么尽量把循环次数更大的放在内层的原因

###### 空间局限性

程序即将用到的指令/数据可能与目前正在用到的指令/数据在空间上相邻或接近。因此处理器在处理当前指令或数据时候，可以从内层中把相邻区域的指令或数据预取到cache中。这就是VPP向量报文的秘密。



###### cache line

cache，中译名高速缓冲存储器，其作用是为了更好的利用局部性原理，减少CPU访问主存的次数。简单地说，CPU正在访问的指令和数据，其可能会被以后多次访问到，或者是该指令和数据附近的内存区域，也可能会被多次访问。因此，第一次访问这一块区域时，将其复制到cache中，以后访问该区域的指令或者数据时，就不用再从主存中取出。

![img](https://img-blog.csdn.net/20180606100828821?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIxMTI1MTgz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

cache分成多个组，每个组分成多个行，**行 linesize**是cache的基本单位，从主存向cache迁移数据都是按照linesize为单位替换的。比如linesize为32Byte，那么迁移必须一次迁移32Byte到cache。 这个linesize比较容易理解，想想我们前面书的例子，我们从书架往书桌搬书必须以书为单位，肯定不能把书撕了以页为单位。书就是linesize。当然了现实生活中每本书页数不同，但是同个cache的linesize总是相同的。

所谓8路组相连（ 8-way set associative）的含义是指，每个组里面有8个行。

我们知道，cache的容量要远远小于主存，主存和cache肯定不是一一对应的，那么主存中的地址和cache的映射关系是怎样的呢？

拿到一个地址，首先是映射到一个组里面去。如何映射？取内存地址的中间几位来映射。

举例来说，data cache: 32-KB, 8-way set associative, 64-byte line size

Cache总大小为32KB，8路组相连（每组有8个line），每个line的大小linesize为64Byte,OK，我们可以很轻易的算出一共有32K/8/64=64 个组。

对于32位的内存地址，每个line有2^6 = 64Byte，所以地址的【0，5】区分line中的那个字节。一共有64个组。我们取内存地址中间6为来hash查找地址属于那个组。即内存地址的【6，11】位来确定属于64组的哪一个组。组确定了之后，【12，31】的内存地址与组中8个line挨个比对，如果【12，31】为与某个line一致，并且这个line为有效，那么缓存命中。

OK，我们可以将cache分成三类，

直接映射高速缓存，这个简单，即每个组只有一个line，选中组之后不需要和组中的每个line比对，因为只有一个line。
组相联高速缓存，这个就是我们前面介绍的cache。 S个组，每个组E个line。

　全相联高速缓存，这个简单，只有一个组，就是全相联。不用hash来确定组，直接挨个比对高位地址，来确定是否命中。可以想见这种方式不适合大的缓存。想想看，如果4M 的大缓存　linesize为32Byte，采用全相联的话，就意味着4*1024*1024/32 = 128K 个line挨个比较，来确定是否命中，这是多要命的事情。高速缓存立马成了低速缓存了。

[]: https://blog.csdn.net/qq_21125183/article/details/80590934



##### 逻辑核和物理核



###### 基础概念

CPU(Central Processing Unit): 中央处理单元，CPU不等于物理核，更不等于逻辑核。
物理核(physical core/processor): 可以看的到的，真实的cpu核，有独立的电路元件以及L1,L2缓存，可以独立地执行指令。
逻辑核(logical core/processor，LCPU): 在同一个物理核内，逻辑层面的核。（比喻，像动画片一样，我们看到的“动画”，其实是一帧一帧静态的画面，24帧/s连起来就骗过了人类的眼睛，看起来像动起来一样。逻辑核也一样，物理核通过高速运算，让应用程序以为有两个cpu在运算）。
超线程(Hyper-threading， HT)：超线程可以在一个逻辑核等待指令执行的间隔(等待从cache或内存中获取下一条指令)，把时间片分配到另一个逻辑核。高速在这两个逻辑核之间切换，让应用程序感知不到这个间隔，误认为自己是独占了一个核。
关系: 一个CPU可以有多个物理核。如果开启了超线程，一个物理核可以分成n个逻辑核，n为超线程的数量。



##### SMP

SMP模式将多个处理器与一个集中的存储器相连。在SMP模式下，所有处理器都可以访问同一个系统物理存储器，这就意味着SMP系统只运行操作系统的一个拷贝。因此SMP系统有时也被称为一致存储器访问（UMA）结构体系，一致性意指无论在什么时候，处理器只能为内存的每个数据保持或共享唯一一个数值。很显然，SMP的缺点是可伸缩性有限，因为在存储器接口达到饱和的时候，增加处理器并不能获得更高的性能。

##### NUMA

numa把一台计算机分成多个节点(node),每个节点内部拥有多个CPU，节点内部使用共有的内存控制器，节点之间是通过互联模块进行连接和信息交互。
因此节点的所有内存对于本节点所有的CPU都是等同的，对于其他节点中的所有CPU都不同。因此每个CPU可以访问整个系统内存，但是访问本地节点的内存速度
最快(不经过互联模块),访问非本地节点的内存速度较慢(需要经过互联模块)，即CPU访问内存的速度与节点的距离有关，该距离成为Node Distance。
查看当前numa的节点情况：
numactl --hardware

节点之间的距离(Node Distance)指从节点1上访问节点0上的内存需要付出的代价的一种表现形式。
Numa内存分配策略有一下四种:
缺省default:总是在本地节点分配(当前进程运行的节点上)。
绑定bind:强制分配到指定节点上。
交叉interleavel:在所有节点或者指定节点上交叉分配内存。
优先preferred:在指定节点上分配，失败则在其他节点上分配。
查看当前系统numa策略：
numactl --show
因为numa默认的内存分配策略是优先在进程所在CPU的本地内存中分配，会导致CPU节点之间内存分配不均衡，
当某个CPU节点内存不足时，会导致swap产生，而不是从远程节点分配内存，这就是swap insanity现象。
MySQL服务器为什么需要关闭numa?
MySQL是单进程多线程架构数据库，当numa采用默认内存分配策略时，MySQL进程会被并且仅仅会被分配到numa的一个节点上去。
假设这个节点的本地内存为10GB,而MySQL配置20GB内存，超出节点本地内存部分(20GB-10GB)Linux会使用swap而不是使用其他节点的物理内存。
在这种情况下，能观察到虽然系统总的可用内存还未用完，但是MySQL进程已经开始在使用swap了。
如果单机只运行一个MySQL实例，可以选择关闭numa，关闭nuam有两种方法：
1.硬件层，在BIOS中设置关闭;
2.OS内核，启动时设置numa=off。
修改/etc/grub.conf文件，在kernel那行追加numa=off;
[root@node130 ~]# vim /boot/grub/grub.conf
title Red Hat Enterprise Linux (2.6.32-358.el6.x86_64)
    root (hd0,0)
    kernel /vmlinuz-2.6.32-358.el6.x86_64 ro root=UUID=cb7d8bdc-28a5-4dbd-b04a-3ad9ee3e6bba rd_NO_LUKS rd_NO_LVM LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM rhgb quiet numa=off
保存后重启服务器，再次检查numa只剩下一个节点就成功了：
[root@node130 ~]# numactl --hardware
available: 1 nodes (0)
node 0 cpus: 0
node 0 size: 2047 MB
node 0 free: 1514 MB
node distances:
node  0
 0: 10

##### NUMA和UMA区别

NUMA通过提供分离的存储器给各个处理器，避免当多个处理器访问同一个存储器产生的性能损失来试图解决这个问题。对于涉及到分散的数据的应用（在服务器和类似于服务器的应用中很常见），NUMA可以通过一个共享的存储器提高性能至n倍,而n大约是处理器（或者分离的存储器）的个数。

当然，不是所有数据都局限于一个任务，所以多个处理器可能需要同一个数据。为了处理这种情况，NUMA系统包含了附加的软件或者硬件来移动不同存储器的数据。这个操作降低了对应于这些存储器的处理器的性能，所以总体的速度提升受制于运行任务的特点。

 

​		因为是对内存访问的差异，所以到底是NUMA好还是UMA好，是要看具体应用的。如果应用本身是每个CPU（或者CPU内部的单个核心）长时间持续运行一个线程，那么NUMA把这个线程用到的数据尽可能放到这个CPU的本地内存中当然会更有性能优势。如果是把大量的数据加载到内存中，根据外部请求创建不同的线程随机访问内存中的一部分数据，短时间的处理后线程结束，则是UMA的平均延迟会更低。一般来说，前者通常是一些运算量很大的应用；后者通常是各种服务器（例如网站、数据库等）。这也是为什么Linux内核允许配置为NUMA或者UMA两种模式，多路系统的BIOS通常允许配置为NUMA模式或者UMA模式的原因。



##### 线程和cpu

###### 发生线程切换

​	x86是不会的，不管是虚拟寻址的tlb还是物理寻址的cache都不会。

因为线程共享进程的线性地址

空间，所以不需要清tlb。进程切换会清tlb，因为会涉及到地址空间的切换。

据我所知mips也不会，arm就不清楚了



##### POSIX Thread Library (NPTL)





##### 协程概念



##### 









#### 计算机网络原理

##### 交换原理

1.交换机根据收到数据帧中的源MAC地址建立该地址同交换机端口的映射，并将其写入**MAC地址表**中。<!--CAM表示cisco 二层交换机维护的一张表-->

2.交换机将数据帧中的目的MAC地址同已建立的MAC地址表进行比较，以决定由哪个端口进行转发。

3.如数据帧中的目的MAC地址不在MAC地址表中，则向所有端口转发。这一过程称为泛洪（flood）.

4.广播帧和组播帧向所有的端口转发。



泛洪（flood）

```
00:23:42:751635: l2-input
  l2-input: sw_if_index 3 dst 33:33:00:00:00:fb src 02:fe:f1:fb:c7:2e
00:23:42:751639: l2-learn
  l2-learn: sw_if_index 3 dst 33:33:00:00:00:fb src 02:fe:f1:fb:c7:2e bd_index 1
00:23:42:751643: l2-flood
  l2-flood: sw_if_index 3 dst 33:33:00:00:00:fb src 02:fe:f1:fb:c7:2e bd_index 1
  l2-flood: sw_if_index 3 dst 33:33:00:00:00:fb src 02:fe:f1:fb:c7:2e bd_index 1


```



交换机本身接口的MAC在二层中不起作用，fib表记录的是mac和接口的对应关系，一个接口可以对应多个mac地址

```
DBGvpp# show l2fib  verbose
    Mac-Address     BD-Idx If-Idx BSN-ISN Age(min) static filter bvi         Interface-Name        
 02:fe:f1:fb:c7:2e    1      3      0/1      -       -      -     -               tap0             
 00:00:00:00:00:01    1      1      0/1      -       -      -     -       VirtualEthernet0/0/0     
L2FIB total/learned entries: 2/2  Last scan time: 0.0000e0sec  Learn limit: 4194304 
```

##### 解决问题思路

###### DPDK-VPP 一次mbuf地址被踩的定位思路

https://blog.csdn.net/sjin_1314/article/details/106090582



##### 算法与数据结构



###### 局部敏感性hash